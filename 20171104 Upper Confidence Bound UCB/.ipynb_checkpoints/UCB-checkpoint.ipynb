{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化學習(Reinforcement Learning)是機器學習的一個分支，也稱為線上學習、評等學習。 它用於解決交互問題，在時間&ensp;$t$&ensp;觀察到的數據被認為決定在時間&ensp;$t + 1$&ensp;採取的行動。它也被用於人工智能，例如最近的Alpha Go，就是透過反覆試驗學習後更新其行為準則，而作者是在章節中使用機器狗的例子。<br> \n",
    "而課程中有介紹其中兩種：<br>\n",
    "+ Upper Confidence Bound (UCB)。<br>\n",
    "+ Thompson Sampling。<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandit Problem(多臂吃角子老虎問題)<Hr>\n",
    "  [吃角子老虎機問題補充1](https://blog.yoctol.com/%E8%B3%AD%E5%BE%92%E7%9A%84%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A71-%E5%90%83%E8%A7%92%E5%AD%90%E8%80%81%E8%99%8E-bandit-%E5%95%8F%E9%A1%8C-62da60b58e3e)<br>\n",
    "  [吃角子老虎機問題補充2](https://zhuanlan.zhihu.com/p/21388070)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">情境說明</span>：有一天，當有一個賭客進入了一間賭場，想要透過吃角子老虎機以小博大，但發現在賭場中居然有一排一模一樣並且擺在一起的吃角子老虎機，他不知道應該選哪一台吃角子老虎機好，因为賭客曾聽人過有些機器中獎機率比較高，有些中獎機率比較低。那麼請問，假如賭客今天有1000塊錢，玩一次需要支付1塊錢，應該怎麼玩可以讓賭客獲得最大的收益?<br>\n",
    "### <span style=\"color:BLUE\">概念</span>：\n",
    "&ensp;&ensp;因為同時有很多台吃角子老虎機供選擇，每一台機器可以得到的期望報酬&ensp;(Expected Reward，指的是玩吃角子老虎機非常多次後得到的平均報酬)&ensp;皆不一樣。站在賭客的立場，目標應該是透過對機器的選擇，從遊戲中獲得最大「期望報酬」。<br>\n",
    "&ensp;&ensp;而剛開始賭客沒有任何各機器的期望報酬資訊，因此賭徒需要<span style=\"color:red\">探勘</span> &ensp;(exploration) &ensp;各個機台報酬的可能性，也就是先試玩一段時間。在探勘累積了足夠次數，對於每台機器的期望報酬有了一定的了解之後，賭徒就可以開始進行<span style=\"color:red\">開發</span> &ensp; (exploitation) &ensp;，不斷去玩最有潛力（期望報酬最高）的機台，獲取最大的累積報酬。<br>\n",
    "(ps：被稱為<span style=\"color:BLUE\">多臂吃角子老虎機</span> (Multi-Armed Bandit)的原因，因為吃角子老虎機的遊戲方式是拉桿，所以會將吃角子老虎機稱為臂(Arm)，指的則是很多台吃角子老虎機給玩家選擇。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/UCB1_1.PNG)\n",
    "在這裡會假設報酬或是獎勵的產生方式是&ensp;<span style=\"color:red\">隨機式</span>(stochastic bandit)&ensp;: $臂_i$的報酬服從某種固定的機率分配$D_i$，並假設每台機器機率分配不同以及事先不知道每個機器的機率分配。\n",
    "\n",
    "![](images/UCB1.PNG)\n",
    "在此例的五台機器中，對我們最有利的是最後一台，因為他的報酬分布較高，這台機器是左偏分布，所以如果你事先知道最後一台的期望報酬最高，你就會一直玩這台機器已獲得最高的報酬，但因為你並不知道，所以你會陷入探勘&開發兩難(exploration and exploitation tradeoff)：因為探勘過多意味著不能獲得較高的收益，而開發過多意味著可能錯過更高回報的機會，所以要利用強化學習(Reinforcement Learning)來解決此問題。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "課程中提到的[Paper](Using Confidence Bounds for Exploitation-Exploration Trade-offs.pdf)。<br>\n",
    "![](images/UCB2.PNG)\n",
    "在生活中，多臂吃角子老虎機(Multi-Armed Bandit)這個問題最常被用在廣告文案的選擇上，當有不同的活動圖片時，哪一種可以吸引較多的點擊。<br>\n",
    "當然可以透過市場調查或是&ensp;$ABTest$&ensp;，了解消費者對於不同設計、顏色的想法，選出一個消費者最喜歡的作為最終呈現。然而這樣的方法有兩個問題，首先，如果每一個小設計都需要蒐集 200 份問卷，或是邀請 20 個消費者進行訪談，耗時且成本過高。此外，消費者的答案是很容易被問卷或訪談所誘導的，因此可能並不能得到真正會吸引消費者的文案。<br>\n",
    "而&ensp;$ABTest$&ensp;，需要兩兩配對進行測試，所以當文案太多的時候會需要很多的樣本去證明那一種文案較佳，同樣耗時且成本過高。<br>\n",
    "所以市場調查或是&ensp;$ABTest$&ensp;僅單純的用來<span style=\"color:red\">探勘</span>&ensp;(exploration) &ensp;找出較佳的結果，而不用來&ensp;<span style=\"color:red\">開發</span>&ensp; (exploitation) 。<Hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper Confidence Bound (UCB)<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 課程中彙總了多臂吃角子老虎機(Multi-Armed Bandit)的問題：(利用廣告文案當範例)\n",
    "![](images/UCB3.PNG)\n",
    "+ 我們有 &ensp;$D$ &ensp;個臂(Arms)，而每個臂代表我們在客戶每次連結網頁時所顯示的廣告。\n",
    "+ 客戶每連結網頁一次，就當作一輪(Round)。\n",
    "+ 總共執行 &ensp;$n$ &ensp;輪，在每一輪選擇一種廣告給客戶。\n",
    "+ 總共執行 &ensp;$n$ &ensp;輪，在每輪選擇的廣告&ensp;$i$ &ensp;去計算每輪的報酬 &ensp;$r_i(n)\\in \\lbrace 0,1 \\rbrace $，當 &ensp;$r_i(n)=1$ &ensp;時代表<span style=\"color:red\">有</span>點擊該廣告 &ensp;$i$ &ensp;，當 &ensp;$r_i(n)=0$ &ensp;時代表<span style=\"color:red\">沒有</span>點擊該廣告 &ensp;$i$。\n",
    "+ 我們的目標是在執行很多輪後，可以獲得最大的報酬(最多的點擊數)。<br>\n",
    "#### 在介紹完要處理的問題後，切入章節的解決方案：Upper Confidence Bound(UCB)\n",
    "![](images/UCB4.PNG)\n",
    "&ensp; 分成三個步驟去完成<br><br>\n",
    "$Step$ &ensp;$1.$：在總共執行 &ensp;$n$ &ensp;輪中，需要先考慮每種廣告&ensp;$i$ &ensp;中兩個數字：\n",
    "1. $N_i(n)$：在總共執行 &ensp;$n$ &ensp;輪中，第&ensp;$i$ &ensp;種廣告被選中的次數。<br>\n",
    "2. $R_i(n)$：在總共執行 &ensp;$n$ &ensp;輪中，第&ensp;$i$ &ensp;種廣告的總報酬數(Sum of reward)。<br><br>\n",
    "$Step$ &ensp;$2.$：根據上面兩個數字去計算$\\overline{r_i}(n)$和信賴區間<br>\n",
    "+ 計算在總共執行 &ensp;$n$ &ensp;輪中，第&ensp;$i$ &ensp;種廣告的平均報酬&ensp;$$\\overline{r_i}(n)=\\frac{R_i(n)}{N_i(n)}$$<br>\n",
    "+ 在總共執行 &ensp;$n$ &ensp;輪中，第&ensp;$i$ &ensp;種廣告的信賴區間<br>\n",
    "$$[\\overline{r_i}(n) - \\Delta_i(n),\\overline{r_i}(n) +\\Delta_i(n)]$$<br>\n",
    "$$\\Delta_i(n)=\\sqrt{\\frac{3}{2} \\cdot \\frac{\\log(n)}{N_i(n)}}$$<br><br>\n",
    "$Step$ &ensp;$3.$：最後我們會在第 &ensp;$n$ &ensp;輪中，算出第$i$廣告的信賴區間上界(UCB) &ensp;$\\overline{r_i}(n) +\\Delta_i(n)$ ，並選取擁有最大信賴區間上界的那個廣告作為第&ensp;$n$ &ensp;輪所要提供的廣告。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "接下來課程還是回歸吃角子老虎機的問題來進行說明\n",
    "![](images/UCB1_1.PNG)\n",
    "![](images/UCB1.PNG)\n",
    "因為事前無法得知那一台機器的報酬分布較高，所以一開始會用相同的期望報酬進行假設，也就是紅色的虛線。\n",
    "![](images/UCB5.PNG)\n",
    "利用假設的期望報酬去畫出一個信賴區間，並請注意一開始要設定信賴區間的時候要設定一個很大的值來用到將真實的信賴區間上界包含進去，因為它會收斂變小。\n",
    "![](images/UCB6.PNG)\n",
    "首先第一輪隨機挑到了第三台機器，而他沒有獲得報酬，所以平均報酬&ensp;$\\overline{r_i}(n)=\\frac{R_i(n)}{N_i(n)}$下降，所以信賴區間會向下移動，而因為該機器被選到了，讓$\\Delta_i(n)=\\sqrt{\\frac{3}{2} \\cdot \\frac{\\log(n)}{N_i(n)}}$也變小了，所以信賴區間縮小了。\n",
    "![](images/UCB7.PNG)\n",
    "首先第二輪隨機挑到了第四台機器，而他有獲得報酬，所以平均報酬&ensp;$\\overline{r_i}(n)=\\frac{R_i(n)}{N_i(n)}$上升，所以信賴區間會向上移動，而當被選到的次數越多，讓$\\Delta_i(n)=\\sqrt{\\frac{3}{2} \\cdot \\frac{\\log(n)}{N_i(n)}}$會變小了，所以信賴區間縮更小。\n",
    "![](images/UCB8.PNG)\n",
    "因為機器的報酬是機率分布的，所以也有可能在回應機率比較低的機台上有報酬產生，只能可能性較低。\n",
    "![](images/UCB9.PNG)<br>\n",
    "![](images/UCB10.PNG)<br>\n",
    "![](images/UCB11.PNG)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
