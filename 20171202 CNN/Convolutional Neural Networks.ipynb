{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Convolutional Neural Networks<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷積神經網路（Convolutional Neural Networks，CNN）\n",
    "在這個章節中，我們將學習：\n",
    "#### 什麼是卷積神經網路（Convolutional Neural Networks，CNN），比較類神經網路和人腦的圖像辨識。\n",
    "#### 分成幾個部份去將卷積類神經網路進行拆解，課程將**卷積神經網路（Convolutional Neural Networks，CNN）** 拆成四個步驟，其中第一個部分又分成兩個部分：\n",
    "+ Step1 **Convolution Operation(卷積運算)**：其中包含**feature detectors(特徵檢測器)**，**filters(濾波器)**，**feature maps(特徵圖)**。<br>\n",
    "+ Step1(B) **RELU-Layer(線性整流單元層)**：介紹**Rectified Linear Unit(線性整流單元 &ensp; RELU)**並且討論圖像辨識(image recognition)為什麼需要非線性的過程。<br>\n",
    "+ Step2 **Pooling(池化)**：介紹什麼是池化以及為什麼需要池化?<br>\n",
    "+ Step3 **Flattening(平化)**：介紹平化的觀念。<br>\n",
    "+ Step4 **Full Connection(全連結)**：介紹全連結層（fully connected layers）的觀念。<br>\n",
    "最後課程中進行了彙總和補充的&ensp; **Softmax** &ensp; 與 &ensp; **Cross-Entropy**。<hr>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what are convolutional neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/CNN1_1.PNG)\n",
    "首先，這是一張人臉圖，當你用曲線右邊去看這張圖，你會認為這是面對右邊的人在斜眼看你，如果用左邊來看，你會覺得是一個人在面對你，在人腦處理圖像或影像的時候，是根據看到的特徵去做辨識，這就是人腦在辨識一張圖的邏輯，你不會接收全部的資訊，你會被部分特徵所吸引下判斷，而找尋特徵去做辨識就是圖像辨識的概念。<br>\n",
    "卷積神經網路（Convolutional Neural Networks，CNN）最簡單的運作架構如圖所示：<br>\n",
    "![](images/CNN1.PNG)\n",
    "輸入了一張圖像，透過CNN獲得標籤(Label)，輸出圖像的分類。\n",
    "![](images/CNN2.PNG)\n",
    "還有一個臉部表情與動作的例子，給CNN一個真實微笑的人的圖像，他會辦別這個人是開心的，而給一張皺著眉頭的臉就判斷她心情差，CNN可以辨識情感，根據提取到的特徵去進行辨識。\n",
    "![](images/CNN3.PNG)\n",
    "用一個基本程度的例子說明如何辨識這些特徵\n",
    "有兩張圖像，一張是黑白像素也被稱為[灰階](https://zh.wikipedia.org/wiki/%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F)(Gray scale)的$2\\times2$的2維陣列的圖像，一張是彩色圖像也被稱為RGB及CMYK，這裡講的是[RGB圖像](https://zh.wikipedia.org/wiki/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F)而每一個的像素是介於0到255之間，共256階。<br>\n",
    "<span style=\"color:red\">補充</span> ：\n",
    "+ 灰階（Gray scale）：用於顯示的灰階圖像通常用每個採樣像素8 bit來保存，這樣可以有256種灰階（8bits就是2的8次方=256），每個點由從 0 (黑色)到 255 (白色)的亮度值來表現，其中間的值來表現不同程度的灰。\n",
    "+ RGB 圖像：代表紅(Red)-綠(Ggreen)-藍(Blue)也就是色光三原色，代表圖像上的每個點用一個「紅色」色階，一個「綠色」色階和一個「藍色」色階表示。人類能辨別的每種顏色都能用紅，綠和藍組合來表現，每一個顏色通道都有 256 種可能的亮度程度。\n",
    "灰度圖像和RGB 圖像最本質的區別就是它們「<span style=\"color:red\">顏色</span> 」的數量：一個灰度圖像只有一個；一個 RGB 圖像有三個。一個 RGB 圖像可以認為是三個灰度圖像的疊加，一個為紅色，一個為綠色，另一個為藍色。\n",
    "![](images/CNN4.PNG)\n",
    "笑臉圖可以轉換成計算機術語表示，而在這邊先忽略灰階，用最簡單的方式表達，黑色用1表示，白色用0表示，就會變成第三張圖，接下來我們會利用第三張圖的部份去當作我們CNN的輸入圖像，更多的細節可以參考作者提供的[論文](paper/Gradient-Based Learning Applied to Document Recognition.pdf)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷積類神經網路拆解：\n",
    "+ Step1 **Convolution Operation(卷積運算)**<br>\n",
    "+ Step1(B) **RELU-Layer(線性整流單元層)**<br>\n",
    "+ Step2 **Pooling(池化)**<br>\n",
    "+ Step3 **Flattening(平化)**<br>\n",
    "+ Step4 **Full Connection(全連結)**<br>\n",
    "\n",
    "### step1 **Convolution Operation(卷積運算)**<Br>\n",
    "$$ (f \\ast g)(t)  \\overset{def}{=} \\int_{-\\infty}^{\\infty} f(\\tau)g(t-\\tau)d\\tau$$   \n",
    "這是卷積(convolution)的數學算式，不多贅述，如果有興趣可以參考作者推薦的[論文](paper/CNN.pdf)，直接用一個簡單的例子來說明怎麼計算。<br>\n",
    "補充[參考文獻](https://chtseng.wordpress.com/2017/09/12/%E5%88%9D%E6%8E%A2%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF/)\n",
    "![](images/CNN5.PNG)\n",
    "這個$input$圖像是延續前面的簡單範例，以及有一個$3\\times3$的陣列稱為特徵檢測器(feature detectors)或是卷積核(kernel)或是濾波器(filters)，但不一定要$3\\times3$，也可以$5\\times5$或$7\\times7$等不同的設定，但最為常見的還是$3\\times3$。<br>\n",
    "進行卷積運算時，會將特徵檢測器放入圖像的<span style=\"color:red\">局部</span>，進行特徵和圖像局部的相符程度計算，只要將兩者對應的各個像素上的值相乘後加總。而每次移動過濾器的距離稱為**間隔（stride）**，在這裡的間隔為**1個像素**，而間隔的移動也是可以改變的。<br>\n",
    "最右邊的圖被稱為特徵圖(feature maps)或是激活圖(activation map)，當透過特徵檢測器的轉換後，原本的$input$圖像會縮小，而縮小的幅度會受到**間隔**的影響，間隔越大圖像會縮得更小，特徵檢測器在卷積過程中最重要的一點就是要讓圖變小因為這樣可以更快比較更容易處理圖像，但問題是我們會損失資訊，但特徵檢測器的目的就是要檢測那些特徵的圖像是不可或缺的，因為特徵檢測器上有固定的圖像特徵，那在特徵圖上越大的數值代表更匹配這個圖案特徵，像數值4的那個部分就是完美對應此特徵檢測器。<br>\n",
    "在本章節開頭時我們討論如何示辨識圖像，在真實世界中我們不可能去看每一個單一的像素，我們在辨識是否為人像是去看鼻子 眼睛等特徵去辨識，而不會去看全部的特徵，不會去接收全部的資訊。\n",
    "![](images/CNN6.PNG)\n",
    "在回到我們的$input$ 圖像，我們因為有不同的濾波器而產生了很多不同的特徵圖，後續會訓練模型找出對於某些分類重要的特徵圖。<br>\n",
    "接著作者在這邊介紹了幾種在[GIMP](https://docs.gimp.org/en/plug-in-convmatrix.html)的濾波器會對原本的圖像做處理分別是：銳化($Sharpen$)、模糊($Blur$)、邊緣增強($Edge enhance$)、邊緣檢測($Edge detect$)、凸印($Emboss$)，是一個用來調整圖像的免費工具。<br>\n",
    "卷積的重點在初始圖像利用特徵檢測器產出特徵圖找到特徵，特徵是神經網路用來檢測與辨識圖像的工具。<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step1(B) **RELU-Layer(線性整流單元層)**<br>\n",
    "  在卷積層之後，會進入線性整流單元(Rectified Linear Unit，ReLU)，這個部分就是利用前面ANN提到的整流函數(Rectified Function)，有些作者會講將卷積層與線性整流單元拆分成兩個步驟，而這裡就合併一起來看。  \n",
    "  ![](images/CNN7.PNG)\n",
    "  使用整流函數的原因是為了在CNN或圖像中增加非線性，而增加非線性原因是因為圖像是高度非線性的，特別是如果你辨識不同的項目時，會有不同的背景或是顏色。\n",
    "  ![](images/CNN8.PNG)\n",
    "  作者補充了一個圖，作者提供的圖[原始來源](paper/ReLU補充.pdf)，下方則是將此圖做**特徵檢測**與**線性整流**。\n",
    "  ![](images/CNN8_1.PNG)\n",
    "  左圖為做完**特徵檢測器**後產生的灰階特徵圖，灰階特徵圖是從黑到白、暗到亮，其中黑色為負，白色為正，不單單只有0,1的區別。<br>\n",
    "  右圖則是透過**整流函數**後將黑色的部分(<span style=\"color:red\">負值</span>)轉換成**$0$**，因為色彩從暗到亮是一個線性的進程，所以如果你拿掉了黑色的部分就會打破線性，因為黑色部分是陰影，所以在特徵圖的階段會依循白灰黑灰白漸層的順序，但加入線性整流單元後就可以打破這個狀態。<br>\n",
    "  作者補充文獻：<br>\n",
    "  [Understanding Convolutional Neural Networks with A Mathematical Model](paper/了解CNN的數學模型.pdf)<br>\n",
    "[Delving Deep into Rectifiers:Surpassing Human-Level Performance on ImageNet Classification](paper/深入整流器.pdf)  \n",
    "  <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2 **Pooling(池化)**<br>\n",
    "![](images/CNN9.PNG)\n",
    "第一張圖是直視，第二章被旋轉，第三章被擠壓，希望透過神經網路可以正確辨識這三張圖。\n",
    "![](images/CNN10.PNG)\n",
    "當然我們還有很多不同類型的豹子圖像，每一張的臉面對的可能是不同的方向、光線不同、背景不同等等，但豹子會有一些獨特的特徵，有一條像是眼淚的條紋。<br>\n",
    "+ 那什麼是池化(Pooling)：池化就是要讓某些特徵具有**空間不變性(spatial invariance)**的屬性，就算有一點點的不同也不會被影響、具有靈活性的特徵，是一個壓縮圖片並保留重要資訊的方法。\n",
    "+ 池化的種類：有很多種不同的池化方法：平均池化(Mean pooling)、最大池化(MAX pooling)等等，其中「最大池化（Max pooling）」是最常見。\n",
    "![](images/CNN12.PNG)\n",
    "+ 池化怎麼運作：首先，產生一個固定大小的矩陣，這個矩陣可以自己選擇大小，間隔也可以自己選擇，較長使用**2$\\times$2**的矩陣、**間隔2**的設定，這裡是用最大池化，就是找出每一格矩陣中最大的值如圖。\n",
    "![](images/CNN11.PNG)\n",
    "因為**2$\\times$2**的矩陣中只保留1個，所以我們減少了75%較不重要的資訊，就算圖像被旋轉了，重要的部分還是會被保留下來，原圖經過池化以後，其所包含的像素數量會降為原本的四分之一，但因為池化後的圖片包含了原圖中各個範圍的最大值，它還是保留了每個範圍和各個特徵的相符程度。也就是說，池化後的資訊更專注於圖片中是否存在相符的特徵，而非圖片中哪裡存在這些特徵。這能幫助 CNN 判斷圖片中是否包含某項特徵，而不必分心於特徵的位置。<br>\n",
    "池化的主要作用有兩個：\n",
    "+ 降低卷積層$output$特徵圖的維度。\n",
    "+ 減少過度擬合($Overfitting$)，過度擬合會發生對於輸入的某些誤差過於敏感，通過池化可以降低雜訊。<br>\n",
    "而為什麼使用**2$\\times$2**的矩陣、**間隔2**的設定，作者提供的[文獻](paper/Evaluation of Pooling Operations.pdf)<br>\n",
    "![](images/CNN13.PNG)\n",
    "所以這是我們現在執行到的流程。\n",
    "![](images/CNN14.PNG)\n",
    "這是作者補充的一個工具，用來辨識數字的，從圖中可以看到完整的CNN過程，如果想看更細的說明可以看課程影片中的STEP2 **Pooling**9分50秒開始的影片。<br>\n",
    "Source:scs.ryerson.ca/~aharley/vis/conv/flat.html<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3**Flattening(平化)**<br>\n",
    "這個步驟是將池化的結果轉換成一行，因為後續要將池化的結果放入神經網路中。\n",
    "![](images/CNN15.PNG)\n",
    "![](images/CNN16.PNG)\n",
    "![](images/CNN17.PNG)\n",
    "課程在後面提了兩個問題：\n",
    "1.\t為什麼在做平化層處理後，將所有特徵圖放入相同的向量中，不會損失特徵圖的空間結構?<br>\n",
    "ANS:原因是我們在建立特徵圖時，我們提取了空間結構的資訊，找出最為相似的部分，所以越大的數值越反應圖像的空間結構，而在做卷積以及池化的步驟我們都有保留這些特徵，所以重點在每個特徵有沒有被保留在向量中。\n",
    "2.\t為什麼我們不直接將全部圖像的資料都丟入向量中?<br>\n",
    "ANS:如果不經過卷積與池化的步驟的話，我們沒辦法找出圖像中獨特的特徵，就沒辦法順利辨識其他相似的圖。<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step4**Full Connection(全連結)**：<br>\n",
    "全連結層（fully connected layers）也被稱為隱藏層(hidden layers)，但全連結層顧名思義就是前一層所有的神經元都連結到該層全部的神經元，但隱藏層不一定會全部連結在一起，上周的ANN範例也是全連結，但如果層數夠多時，全連結會導致參數量過大，訓練速度緩慢等缺點，所以在多層ANN中通常不會使用權連結的方式，所以不會稱為全連結層。<br>\n",
    "![](images/CNN18.PNG)\n",
    "過去在ANN我們習慣只有一個output，因為是去預測數值型的結果，而在分類上，如果只有兩個分類的結果，可以只用一個ouput以二項式表示0代表一類，1代表另一類，但在多個分類下面就需要每一個分類都建立一個Output。\n",
    "![](images/CNN19.PNG)\n",
    "在經過每次**向前**與**倒傳遞**過程，這些特徵和權重會進行調整，可能調高一點或低一點，誤差也會被重新計算，成功降低誤差的調整將被保留，不斷執行以上步驟，辨認更多已標記的圖片。訓練過程中，個別圖片裡的誤判會慢慢減少，但這些圖片中共通的特徵和權重會留下。\n",
    "![](images/CNN20.PNG)\n",
    "![](images/CNN21.PNG)\n",
    "因為在step3的時候將特徵做平化層處理，所以在最後的全連結層中的每個值(特徵)都可以決定圖片中的狗還是貓，每一個特徵都會有一個根據前面的過程得到一個分數，通常設在0~1之間，之後將這些特徵資訊轉化為票數，由於會有某些獨特的特徵值可以更好地判別狗，有些則更適合用來判斷貓，有些節點就會在辨識某些分類時被忽略掉，如果在訓練過程中，值越高並且常常被判斷到同一個類別中的話，這些值可以投的票數會比其他值還多。所有值對不同選項所投下的票數，將會以權重（weight）或連結強度（connection strength）的方式來表示。\n",
    "![](images/CNN22.PNG)\n",
    "![](images/CNN23.PNG)<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 彙總<br>\n",
    "![](images/CNN24.PNG)\n",
    "![](images/CNN28.PNG)\n",
    "影片中將整個完成流程再順一次：\n",
    "+ 有一個$input$的圖像，我們想要判斷他是什麼樣類型的圖像。\n",
    "+ STEP1 卷積層：利用多個不同的**特徵檢測器**產生出**特徵圖**，接著應用**線性整流單元**移除圖像的線性、增加圖像的非線性。\n",
    "+ STEP2 池化層：在保證**空間不變性**下縮小圖像的大小，並且避免**過度配適**的狀況降低雜訊。\n",
    "+ STEP3 平化層：將池化後的陣列或矩陣轉換成**一列**向量。\n",
    "+ STEP4 全連結層：將平化後的向量透過最後的**全連結層**對分類進行投票。\n",
    "+ 最後透過多次的向前與向後傳遞的迭代進行權重的調整完成整個神經網路的過程，當然過程中除了權重會被調整外，特徵檢測器也會進行調整。<br>\n",
    "作者額外補充[文獻](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 補充：**Softmax&amp;Cross-Entropy**<br>\n",
    "+ [Softmax](https://zh.wikipedia.org/wiki/Softmax%E5%87%BD%E6%95%B0)<br>\n",
    "![](images/CNN25.PNG)\n",
    "簡單來說就是在輸出結果中擁有最大權重(分數)的項對應的分類結果(以此範例為狗)，當你有多個分類結果的時候，凸顯其中最大的值並抑制遠低於最大值的其他分類結果。\n",
    "+ Cross-Entropy<br>\n",
    "![](images/CNN261.PNG)\n",
    "![](images/CNN262.PNG)\n",
    "上面是Cross-Entropy的公式，下面對它的使用情境，以及Cross-Entropy、MSE兩種方法進行比較。\n",
    "![](images/CNN26.PNG)\n",
    "![](images/CNN27.PNG)\n",
    "在比較兩個神經網路上，看起來第一個優於第二個，而比較效能上：\n",
    "+ 比較錯分率，這裡可以看到兩個是一樣的，但這沒辦法真的比較出模型的好壞，所以在倒傳遞中錯分率不是一個適合的方法。<br>\n",
    "+ 比較MSE算是比較好的，可以看出第一個模型較優。<br>\n",
    "+ 比較Cross-Entropy，一樣可以判別出第一個模型較優。<br>\n",
    "那為什麼要用Cross-Entropy呢?<br>\n",
    "假設今天$output$的預測結果比實際值小非常多，那神經網路就會很難透過MSE去調整權重，而Cross-Entropy因為是取log，所以即使是小改變也會有很大的提升，但Cross-Entropy只適用在分類上，如果是數值型(機率值)的結果的話建議去使用MSE。<br>\n",
    "作者補充：<br>\n",
    "[A Friendly Introduction to Cross-Entropy Loss](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)<br>\n",
    "[How to implement a neural network Intermezzo 2](http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python範例<br>\n",
    "### 分成幾個部分：<br>\n",
    "+ 情境說明。<br> \n",
    "+ Part1 建立CNN模型。<br>\n",
    "+ Part2 配適CNN圖像。<hr>\n",
    "\n",
    "#### 情境說明<br>\n",
    "CNN沒有要解決之前的商業問題，而是要分類圖像，分類貓和狗的圖片。\n",
    "首先準備CNN的工作環境，因為和先前的模型相比，因為先前的工作會根據一個table，裡面包含了多個X以及一個Y，而這邊我們要$input$圖像，所以我們要進行圖像的處理。<br>\n",
    "+ 第一種方法是將圖用類別命名，cat1~cat5000、dog1~dog5000然後用程式去對圖像檔名稱進行提取將提取的檔名作為Y的結果。\n",
    "+ 第二種方法是採用keras套件，並且利用資料夾進行分類，分成train資料夾、test資料夾，裡面分別有cats，dogs資料夾，這樣keras套件就會知道每一張圖的實際分類，Train各4000筆，test各1000筆。<br>\n",
    "和最初的資料預處理章節比較，我們不需要建立**X變數**，因為我們的$input$的內容是彩色的**圖像**(RGB)，也不需要分**訓練&amp;測試**資料集，因為資料夾就分好了，但我們需要進行**資料尺度化**的處理，之前有提到在深度學習中必定要做資料尺度化的處理，但會在後面**配適CNN**時才會做，所以和前面的章節不同的地方是不需要先做資料預處理的部分。<hr>\n",
    "\n",
    "#### Part1 建立CNN模型。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Part 1 - Building the CNN\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "將使用這五個套件<br>\n",
    "[keras簡體中文說明](https://keras-cn.readthedocs.io/en/latest/)<br>\n",
    "[keras原文說明](https://keras.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units=128, activation = 'relu'))\n",
    "classifier.add(Dense(units= 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "有兩種方法用來初始化神經網路1.層序列(sequence of layers)。2.圖像(graph)。CNN仍然選擇層序列來初始化神經網路。<br>\n",
    "首先用[Sequential](https://keras.io/getting-started/sequential-model-guide/)類別建立一個classifier物件，Sequential是Keras中的主要模型。<br>\n",
    "\n",
    "使用[add](https://keras.io/layers/merge/#add)方法替神經網路添加不同的層(layer)：\n",
    "#### **STEP1卷積層** 使用[Conv2D](https://keras.io/layers/convolutional/#conv2d)，本次使用參數包括：\n",
    "\n",
    "- **filters(濾波器):**輸出空間的維度（即卷積中濾波器的輸出個數）。\n",
    "- **kernel_size(濾波器大小):**可以是一個整數值(表示長寬設相同的值)或是2個整數的列表。\n",
    "- **input_shape:**在建立第一層卷積層時使用，但要注意圖像的格式，可能是灰階或是彩色，以及大小，需要轉換成相同的格式大小，因為貓狗可能會有不同顏色，所以我們選擇3(RGB)，但受限於CPU所以選擇較小的格式。\n",
    "- **activation(激活函數):**同ANN可參考[activations](https://keras.io/activations/)，然後CNN會選擇線性整流單元(relu)。\n",
    "\n",
    "#### **STEP2池化層**使用[MaxPooling2D](https://keras.io/layers/pooling/#maxpooling2d)，本次使用參數包括：\n",
    "\n",
    "- **pool_size(池化大小):**可以是一個整數值(表示長寬設相同的值)或是2個整數的列表(垂直，水平) 。\n",
    "\n",
    "#### **STEP3平化層**使用[Flatten](https://keras.io/layers/core/#flatten)：<br>\n",
    "用來建立STEP3 平化層將池化的結果轉換成一行，因為後續要將池化的結果放入神經網路。\n",
    "\n",
    "#### **STEP4全連結層**使用[Dense](https://keras.io/layers/core/#dense)建立，代表一個與他層神經元全連接的核心層，本次使用參數包括：\n",
    "\n",
    "- **units:** 該層的神經元個數(憑經驗，太大會計算很久，太小可能分類不理想)。\n",
    "- **activation(激活函數):** 可參考[activations](https://keras.io/activations/)。\n",
    "\n",
    "建立完神經網路的架構後，使用[compile](https://keras.io/models/sequential/)方法來編譯模型，本次使用參數參數包括：\n",
    "\n",
    "- **optimizer:** 求取損失函數最佳解的方法，可參考[optimizers](https://keras.io/optimizers/)。\n",
    "- **loss:** 損失函數，可參考[losses](https://keras.io/losses/)。\n",
    "- **metrics:** 用於評估模型優劣的指標，基本上就是用metrics=['accuracy']。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part2 配適CNN圖像。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part 2 - Fitting the CNN to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
